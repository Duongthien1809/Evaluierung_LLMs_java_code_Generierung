[
{
  "prompt_technik": "ToT",
  "model_name": "llama-guard-3-8b",
  "total_evaluated": 50,
  "completion_evaluation": 0.08,
  "compilable_evaluation": 0.62,
  "code_bleu_average_score.bleu_score": 0.3942710790233106,
  "code_bleu_average_score.ngram_match_score": 0.0040521010075612145,
  "code_bleu_average_score.weighted_ngram_match_score": 0.24636554841901456,
  "code_bleu_average_score.syntax_match_score": 0.32666666666666666,
  "code_bleu_average_score.dataflow_match_score": 0.0,
  "codeBert_average_score.codeBert_average_precision": 0.7148651802539825,
  "codeBert_average_score.codeBert_average_recall": 0.4638916480541229,
  "codeBert_average_score.codeBert_average_f1": 0.5622440135478973
},
{
  "prompt_technik": "ToT",
  "model_name": "llama3-groq-8b-8192-tool-use-preview",
  "total_evaluated": 50,
  "completion_evaluation": 0.08,
  "compilable_evaluation": 0.36,
  "code_bleu_average_score.bleu_score": 0.35475619534656516,
  "code_bleu_average_score.ngram_match_score": 0.1587374323886362,
  "code_bleu_average_score.weighted_ngram_match_score": 0.24024047325473016,
  "code_bleu_average_score.syntax_match_score": 0.4560784906795534,
  "code_bleu_average_score.dataflow_match_score": 0.4839683850633408,
  "codeBert_average_score.codeBert_average_precision": 0.7826733565330506,
  "codeBert_average_score.codeBert_average_recall": 0.7324123644828796,
  "codeBert_average_score.codeBert_average_f1": 0.7559593224525452
},
{
  "prompt_technik": "ToT",
  "model_name": "llama3-8b-8192",
  "total_evaluated": 50,
  "completion_evaluation": 0.1,
  "compilable_evaluation": 0.38,
  "code_bleu_average_score.bleu_score": 0.3816218431519769,
  "code_bleu_average_score.ngram_match_score": 0.2176740423750104,
  "code_bleu_average_score.weighted_ngram_match_score": 0.31379407855654046,
  "code_bleu_average_score.syntax_match_score": 0.5390560563960701,
  "code_bleu_average_score.dataflow_match_score": 0.45596319528028656,
  "codeBert_average_score.codeBert_average_precision": 0.8294959390163421,
  "codeBert_average_score.codeBert_average_recall": 0.7691720533370972,
  "codeBert_average_score.codeBert_average_f1": 0.7976411342620849
},
{
  "prompt_technik": "ToT",
  "model_name": "gemma2-9b-it",
  "total_evaluated": 50,
  "completion_evaluation": 0.02,
  "compilable_evaluation": 0.46,
  "code_bleu_average_score.bleu_score": 0.3778868484019342,
  "code_bleu_average_score.ngram_match_score": 0.2090081009403756,
  "code_bleu_average_score.weighted_ngram_match_score": 0.26911776781869917,
  "code_bleu_average_score.syntax_match_score": 0.5432039330365309,
  "code_bleu_average_score.dataflow_match_score": 0.490217591812131,
  "codeBert_average_score.codeBert_average_precision": 0.8043153667449952,
  "codeBert_average_score.codeBert_average_recall": 0.7953754997253418,
  "codeBert_average_score.codeBert_average_f1": 0.79901393532753
},
{
  "prompt_technik": "ToT",
  "model_name": "llama3-groq-70b-8192-tool-use-preview",
  "total_evaluated": 50,
  "completion_evaluation": 0.14,
  "compilable_evaluation": 0.6,
  "code_bleu_average_score.bleu_score": 0.34384099966444626,
  "code_bleu_average_score.ngram_match_score": 0.1965597212037883,
  "code_bleu_average_score.weighted_ngram_match_score": 0.2532475828558996,
  "code_bleu_average_score.syntax_match_score": 0.5046720140902827,
  "code_bleu_average_score.dataflow_match_score": 0.40088468050781434,
  "codeBert_average_score.codeBert_average_precision": 0.8113428926467896,
  "codeBert_average_score.codeBert_average_recall": 0.7886899697780609,
  "codeBert_average_score.codeBert_average_f1": 0.7991389870643616
},
{
  "prompt_technik": "ToT",
  "model_name": "llama3-70b-8192",
  "total_evaluated": 50,
  "completion_evaluation": 0.08,
  "compilable_evaluation": 0.48,
  "code_bleu_average_score.bleu_score": 0.4181035692186547,
  "code_bleu_average_score.ngram_match_score": 0.26040633501484456,
  "code_bleu_average_score.weighted_ngram_match_score": 0.34466949877900943,
  "code_bleu_average_score.syntax_match_score": 0.5578587659587565,
  "code_bleu_average_score.dataflow_match_score": 0.5094796771220081,
  "codeBert_average_score.codeBert_average_precision": 0.8356449806690216,
  "codeBert_average_score.codeBert_average_recall": 0.784056327342987,
  "codeBert_average_score.codeBert_average_f1": 0.8085364460945129
},
{
  "prompt_technik": "ToT",
  "model_name": "mixtral-8x7b-32768",
  "total_evaluated": 50,
  "completion_evaluation": 0.14,
  "compilable_evaluation": 0.4,
  "code_bleu_average_score.bleu_score": 0.35108273284398495,
  "code_bleu_average_score.ngram_match_score": 0.19431052536527144,
  "code_bleu_average_score.weighted_ngram_match_score": 0.2573902360669699,
  "code_bleu_average_score.syntax_match_score": 0.49574949759099474,
  "code_bleu_average_score.dataflow_match_score": 0.4368806723527037,
  "codeBert_average_score.codeBert_average_precision": 0.8070388007164001,
  "codeBert_average_score.codeBert_average_recall": 0.7931845736503601,
  "codeBert_average_score.codeBert_average_f1": 0.7990796315670013
},
{
  "prompt_technik": "ToT",
  "model_name": "llama-3.1-70b-versatile",
  "total_evaluated": 50,
  "completion_evaluation": 0.06,
  "compilable_evaluation": 0.3,
  "code_bleu_average_score.bleu_score": 0.3985352249683117,
  "code_bleu_average_score.ngram_match_score": 0.2692709798786925,
  "code_bleu_average_score.weighted_ngram_match_score": 0.3271147100919605,
  "code_bleu_average_score.syntax_match_score": 0.5561658225297338,
  "code_bleu_average_score.dataflow_match_score": 0.4415893873728599,
  "codeBert_average_score.codeBert_average_precision": 0.8345960688591003,
  "codeBert_average_score.codeBert_average_recall": 0.8276289129257202,
  "codeBert_average_score.codeBert_average_f1": 0.8301499724388123
},
{
  "prompt_technik": "ToT",
  "model_name": "gemma-7b-it",
  "total_evaluated": 50,
  "completion_evaluation": 0.1,
  "compilable_evaluation": 0.72,
  "code_bleu_average_score.bleu_score": 0.3629149134809821,
  "code_bleu_average_score.ngram_match_score": 0.21139989632043168,
  "code_bleu_average_score.weighted_ngram_match_score": 0.3072036668849086,
  "code_bleu_average_score.syntax_match_score": 0.5026220200493215,
  "code_bleu_average_score.dataflow_match_score": 0.4304340706692665,
  "codeBert_average_score.codeBert_average_precision": 0.8213438475131989,
  "codeBert_average_score.codeBert_average_recall": 0.7832131564617157,
  "codeBert_average_score.codeBert_average_f1": 0.8013881731033325
},
{
  "prompt_technik": "ToT",
  "model_name": "gpt35",
  "total_evaluated": 50,
  "completion_evaluation": 0.36,
  "compilable_evaluation": 0.68,
  "code_bleu_average_score.bleu_score": 0.3313191285972346,
  "code_bleu_average_score.ngram_match_score": 0.2043705591142883,
  "code_bleu_average_score.weighted_ngram_match_score": 0.25897079643749743,
  "code_bleu_average_score.syntax_match_score": 0.481023422574169,
  "code_bleu_average_score.dataflow_match_score": 0.3809117362629837,
  "codeBert_average_score.codeBert_average_precision": 0.7947013998031616,
  "codeBert_average_score.codeBert_average_recall": 0.7817464601993561,
  "codeBert_average_score.codeBert_average_f1": 0.7875713956356049
}
]
