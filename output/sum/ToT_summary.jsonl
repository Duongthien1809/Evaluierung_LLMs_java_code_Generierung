{
  "prompt_technik": "ToT",
  "model_name": "llama3-groq-8b-8192-tool-use-preview",
  "total_evaluated": 50,
  "completion_evaluation": 0.04,
  "compilable_evaluation": 0.18,
  "code_bleu_average_score": {
    "bleu_score": 0.35209871830648043,
    "ngram_match_score": 0.18766932617345194,
    "weighted_ngram_match_score": 0.2865868959706108,
    "syntax_match_score": 0.4468525843531008,
    "dataflow_match_score": 0.48728606672875807
  },
  "codeBert_average_score": {
    "codeBert_average_precision": 0.797597142457962,
    "codeBert_average_recall": 0.7396715116500855,
    "codeBert_average_f1": 0.7670179510116577
  }
}
{
  "prompt_technik": "ToT",
  "model_name": "gemma-7b-it",
  "total_evaluated": 50,
  "completion_evaluation": 0.02,
  "compilable_evaluation": 0.16,
  "code_bleu_average_score": {
    "bleu_score": 0.27756531728217615,
    "ngram_match_score": 0.04584909533385615,
    "weighted_ngram_match_score": 0.06965062568036524,
    "syntax_match_score": 0.11728325448946818,
    "dataflow_match_score": 0.29747829362501493
  },
  "codeBert_average_score": {
    "codeBert_average_precision": 0.6354095757007598,
    "codeBert_average_recall": 0.5513085585832596,
    "codeBert_average_f1": 0.589137670993805
  }
}
{
  "prompt_technik": "ToT",
  "model_name": "llama-guard-3-8b",
  "total_evaluated": 50,
  "completion_evaluation": 0.04,
  "compilable_evaluation": 0.2,
  "code_bleu_average_score": {
    "bleu_score": 0.2533333333333333,
    "ngram_match_score": 0.0,
    "weighted_ngram_match_score": 0.0,
    "syntax_match_score": 0.013333333333333332,
    "dataflow_match_score": 0.0
  },
  "codeBert_average_score": {
    "codeBert_average_precision": 0.5156890714168548,
    "codeBert_average_recall": 0.36943905711174013,
    "codeBert_average_f1": 0.4300091361999512
  }
}
{
  "prompt_technik": "ToT",
  "model_name": "gemma2-9b-it",
  "total_evaluated": 50,
  "completion_evaluation": 0.02,
  "compilable_evaluation": 0.14,
  "code_bleu_average_score": {
    "bleu_score": 0.40708938050803284,
    "ngram_match_score": 0.24717870803490416,
    "weighted_ngram_match_score": 0.3146495739170198,
    "syntax_match_score": 0.5723763831151119,
    "dataflow_match_score": 0.4941528569650954
  },
  "codeBert_average_score": {
    "codeBert_average_precision": 0.8322391033172607,
    "codeBert_average_recall": 0.8394855761528015,
    "codeBert_average_f1": 0.8349226558208466
  }
}
{
  "prompt_technik": "ToT",
  "model_name": "llama3-groq-70b-8192-tool-use-preview",
  "total_evaluated": 50,
  "completion_evaluation": 0.04,
  "compilable_evaluation": 0.12,
  "code_bleu_average_score": {
    "bleu_score": 0.32134094375408506,
    "ngram_match_score": 0.17486397296874182,
    "weighted_ngram_match_score": 0.23173588200728404,
    "syntax_match_score": 0.4789537691440022,
    "dataflow_match_score": 0.3798101508963125
  },
  "codeBert_average_score": {
    "codeBert_average_precision": 0.7956750166416168,
    "codeBert_average_recall": 0.7727120733261108,
    "codeBert_average_f1": 0.7830932283401489
  }
}
{
  "prompt_technik": "ToT",
  "model_name": "gpt35",
  "total_evaluated": 50,
  "completion_evaluation": 0.02,
  "compilable_evaluation": 0.24,
  "code_bleu_average_score": {
    "bleu_score": 0.15221949955386382,
    "ngram_match_score": 0.06755533146746032,
    "weighted_ngram_match_score": 0.09855535101983906,
    "syntax_match_score": 0.2134614049907368,
    "dataflow_match_score": 0.22930591073741913
  },
  "codeBert_average_score": {
    "codeBert_average_precision": 0.7036397707462311,
    "codeBert_average_recall": 0.7237064802646637,
    "codeBert_average_f1": 0.7127399945259094
  }
}
{
  "prompt_technik": "ToT",
  "model_name": "llama-3.1-70b-versatile",
  "total_evaluated": 50,
  "completion_evaluation": 0.0,
  "compilable_evaluation": 0.22,
  "code_bleu_average_score": {
    "bleu_score": 0.21405043526145393,
    "ngram_match_score": 0.014853370649567346,
    "weighted_ngram_match_score": 0.02685230627849716,
    "syntax_match_score": 0.1049822881713032,
    "dataflow_match_score": 0.36951377594644796
  },
  "codeBert_average_score": {
    "codeBert_average_precision": 0.6278703171014786,
    "codeBert_average_recall": 0.5877807068824769,
    "codeBert_average_f1": 0.6039792495965958
  }
}
{
  "prompt_technik": "ToT",
  "model_name": "llama3-70b-8192",
  "total_evaluated": 50,
  "completion_evaluation": 0.0,
  "compilable_evaluation": 0.04,
  "code_bleu_average_score": {
    "bleu_score": 0.24927916983924026,
    "ngram_match_score": 0.019269964527293335,
    "weighted_ngram_match_score": 0.03339187516207891,
    "syntax_match_score": 0.0746501124383764,
    "dataflow_match_score": 0.32980472722921234
  },
  "codeBert_average_score": {
    "codeBert_average_precision": 0.64439945936203,
    "codeBert_average_recall": 0.56424760222435,
    "codeBert_average_f1": 0.6006907856464386
  }
}
