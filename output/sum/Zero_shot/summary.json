[
  {
    "prompt_technik": "Zero_shot",
    "model_name": "llama3-groq-8b-8192-tool-use-preview",
    "total_evaluated": 50,
    "completion_evaluation": 0.08,
    "compilable_evaluation": 0.66,
    "code_bleu_average_score.bleu_score": 0.3862468864991332,
    "code_bleu_average_score.ngram_match_score": 0.20756467039925547,
    "code_bleu_average_score.weighted_ngram_match_score": 0.3085313614504216,
    "code_bleu_average_score.syntax_match_score": 0.5221401704759765,
    "code_bleu_average_score.dataflow_match_score": 0.5067513436708793,
    "codeBert_average_score.codeBert_average_precision": 0.8218525314331054,
    "codeBert_average_score.codeBert_average_recall": 0.7606448483467102,
    "codeBert_average_score.codeBert_average_f1": 0.7895071983337403
  },
  {
    "prompt_technik": "Zero_shot",
    "model_name": "llama3-8b-8192",
    "total_evaluated": 50,
    "completion_evaluation": 0.14,
    "compilable_evaluation": 0.4,
    "code_bleu_average_score.bleu_score": 0.39833083155184634,
    "code_bleu_average_score.ngram_match_score": 0.24938452310099848,
    "code_bleu_average_score.weighted_ngram_match_score": 0.3256713008181385,
    "code_bleu_average_score.syntax_match_score": 0.5480059547105872,
    "code_bleu_average_score.dataflow_match_score": 0.4702615475776611,
    "codeBert_average_score.codeBert_average_precision": 0.8330435800552368,
    "codeBert_average_score.codeBert_average_recall": 0.7800901758670807,
    "codeBert_average_score.codeBert_average_f1": 0.8053966927528381
  },
  {
    "prompt_technik": "Zero_shot",
    "model_name": "gemma-7b-it",
    "total_evaluated": 50,
    "completion_evaluation": 0.1,
    "compilable_evaluation": 0.54,
    "code_bleu_average_score.bleu_score": 0.4033382095957947,
    "code_bleu_average_score.ngram_match_score": 0.26559708961551676,
    "code_bleu_average_score.weighted_ngram_match_score": 0.3557770370220399,
    "code_bleu_average_score.syntax_match_score": 0.5207545311331337,
    "code_bleu_average_score.dataflow_match_score": 0.4712241806124884,
    "codeBert_average_score.codeBert_average_precision": 0.8521346008777618,
    "codeBert_average_score.codeBert_average_recall": 0.8197966182231903,
    "codeBert_average_score.codeBert_average_f1": 0.8352004051208496
  },
  {
    "prompt_technik": "Zero_shot",
    "model_name": "llama3-groq-70b-8192-tool-use-preview",
    "total_evaluated": 50,
    "completion_evaluation": 0.12,
    "compilable_evaluation": 0.48,
    "code_bleu_average_score.bleu_score": 0.3420183368899079,
    "code_bleu_average_score.ngram_match_score": 0.1662527365670478,
    "code_bleu_average_score.weighted_ngram_match_score": 0.2315384339427052,
    "code_bleu_average_score.syntax_match_score": 0.4732297986853247,
    "code_bleu_average_score.dataflow_match_score": 0.3570523783645539,
    "codeBert_average_score.codeBert_average_precision": 0.804310804605484,
    "codeBert_average_score.codeBert_average_recall": 0.7730980479717254,
    "codeBert_average_score.codeBert_average_f1": 0.7871908235549927
  },
  {
    "prompt_technik": "Zero_shot",
    "model_name": "llama-3.1-70b-versatile",
    "total_evaluated": 50,
    "completion_evaluation": 0.08,
    "compilable_evaluation": 0.56,
    "code_bleu_average_score.bleu_score": 0.4024902800808793,
    "code_bleu_average_score.ngram_match_score": 0.2676103971953843,
    "code_bleu_average_score.weighted_ngram_match_score": 0.3311138114165403,
    "code_bleu_average_score.syntax_match_score": 0.5579361755301369,
    "code_bleu_average_score.dataflow_match_score": 0.45330073618145567,
    "codeBert_average_score.codeBert_average_precision": 0.8362521076202393,
    "codeBert_average_score.codeBert_average_recall": 0.8222660577297211,
    "codeBert_average_score.codeBert_average_f1": 0.828159646987915
  },
  {
    "prompt_technik": "Zero_shot",
    "model_name": "llama-guard-3-8b",
    "total_evaluated": 50,
    "completion_evaluation": 0.08,
    "compilable_evaluation": 0.78,
    "code_bleu_average_score.bleu_score": 0.3835799572822998,
    "code_bleu_average_score.ngram_match_score": 0.0040521010075612145,
    "code_bleu_average_score.weighted_ngram_match_score": 0.22872926658317655,
    "code_bleu_average_score.syntax_match_score": 0.30153846153846153,
    "code_bleu_average_score.dataflow_match_score": 0.02,
    "codeBert_average_score.codeBert_average_precision": 0.712771406173706,
    "codeBert_average_score.codeBert_average_recall": 0.469291370511055,
    "codeBert_average_score.codeBert_average_f1": 0.5653769588470459
  },
  {
    "prompt_technik": "Zero_shot",
    "model_name": "llama3-70b-8192",
    "total_evaluated": 50,
    "completion_evaluation": 0.24,
    "compilable_evaluation": 0.6,
    "code_bleu_average_score.bleu_score": 0.4182921690556328,
    "code_bleu_average_score.ngram_match_score": 0.2752423198932442,
    "code_bleu_average_score.weighted_ngram_match_score": 0.3576133139356388,
    "code_bleu_average_score.syntax_match_score": 0.5592060491830152,
    "code_bleu_average_score.dataflow_match_score": 0.48110699321063294,
    "codeBert_average_score.codeBert_average_precision": 0.8398835587501526,
    "codeBert_average_score.codeBert_average_recall": 0.7834269177913665,
    "codeBert_average_score.codeBert_average_f1": 0.8102292621135712
  },
  {
    "prompt_technik": "Zero_shot",
    "model_name": "mixtral-8x7b-32768",
    "total_evaluated": 50,
    "completion_evaluation": 0.14,
    "compilable_evaluation": 0.64,
    "code_bleu_average_score.bleu_score": 0.3902764431150482,
    "code_bleu_average_score.ngram_match_score": 0.2539888292104684,
    "code_bleu_average_score.weighted_ngram_match_score": 0.3068922569977123,
    "code_bleu_average_score.syntax_match_score": 0.5561158006261522,
    "code_bleu_average_score.dataflow_match_score": 0.4441088856258601,
    "codeBert_average_score.codeBert_average_precision": 0.8422551918029785,
    "codeBert_average_score.codeBert_average_recall": 0.8472525179386139,
    "codeBert_average_score.codeBert_average_f1": 0.8437952542304993
  },
  {
    "prompt_technik": "Zero_shot",
    "model_name": "gpt35",
    "total_evaluated": 50,
    "completion_evaluation": 0.58,
    "compilable_evaluation": 0.96,
    "code_bleu_average_score.bleu_score": 0.40127037880511374,
    "code_bleu_average_score.ngram_match_score": 0.251416968461031,
    "code_bleu_average_score.weighted_ngram_match_score": 0.31082481698222036,
    "code_bleu_average_score.syntax_match_score": 0.5893773608437397,
    "code_bleu_average_score.dataflow_match_score": 0.4534623689334637,
    "codeBert_average_score.codeBert_average_precision": 0.8256327688694001,
    "codeBert_average_score.codeBert_average_recall": 0.8099679446220398,
    "codeBert_average_score.codeBert_average_f1": 0.8168431699275971
  },
  {
    "prompt_technik": "Zero_shot",
    "model_name": "gemma2-9b-it",
    "total_evaluated": 50,
    "completion_evaluation": 0.36,
    "compilable_evaluation": 0.9,
    "code_bleu_average_score": {
      "bleu_score": 0.42398999307749413,
      "ngram_match_score": 0.27885424868594066,
      "weighted_ngram_match_score": 0.36911964834103395,
      "syntax_match_score": 0.5665969601024387,
      "dataflow_match_score": 0.4813891151805634
    },
    "codeBert_average_score": {
      "codeBert_average_precision": 0.8530058646202088,
      "codeBert_average_recall": 0.83566725730896,
      "codeBert_average_f1": 0.8434679853916168
    }
  }
]
