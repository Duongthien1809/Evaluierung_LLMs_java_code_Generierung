{
  "model_name": "gpt35",
  "total_evaluated": 50,
  "completion_evaluation": 0.56,
  "compilable_evaluation": 0.8,
  "code_bleu_average_score": {
    "bleu_score": 0.4003540530961833,
    "ngram_match_score": 0.23970258847521764,
    "weighted_ngram_match_score": 0.3271997883711004,
    "syntax_match_score": 0.5755559569991819,
    "dataflow_match_score": 0.4589578785392333
  },
  "codeBert_average_score": {
    "codeBert_average_precision": 0.8336946153640747,
    "codeBert_average_recall": 0.7837279140949249,
    "codeBert_average_f1": 0.8074888360500335
  }
}
{
  "model_name": "EleutherAI/gpt-neo-125m",
  "total_evaluated": 50,
  "completion_evaluation": 0.0,
  "compilable_evaluation": 0.04,
  "code_bleu_average_score": {
    "bleu_score": 0.30631503212399536,
    "ngram_match_score": 0.0064063745205950075,
    "weighted_ngram_match_score": 0.08950195956875139,
    "syntax_match_score": 0.19791938855831145,
    "dataflow_match_score": 0.05143240584832369
  },
  "codeBert_average_score": {
    "codeBert_average_precision": 0.6661835408210754,
    "codeBert_average_recall": 0.5326849281787872,
    "codeBert_average_f1": 0.5905136537551879
  }
}
{
  "model_name": "codeparrot/codeparrot-small",
  "total_evaluated": 50,
  "completion_evaluation": 0.0,
  "compilable_evaluation": 0.0,
  "code_bleu_average_score": {
    "bleu_score": 0.3090696233224551,
    "ngram_match_score": 0.004781347628735965,
    "weighted_ngram_match_score": 0.08283280187478936,
    "syntax_match_score": 0.15533101045296166,
    "dataflow_match_score": 0.013333333333333332
  },
  "codeBert_average_score": {
    "codeBert_average_precision": 0.644965512752533,
    "codeBert_average_recall": 0.5240929585695266,
    "codeBert_average_f1": 0.5778150582313537
  }
}
{
  "model_name": "Salesforce/codegen-2B-mono",
  "total_evaluated": 50,
  "completion_evaluation": 0.0,
  "compilable_evaluation": 0.0,
  "code_bleu_average_score": {
    "bleu_score": 0.31011187241503485,
    "ngram_match_score": 0.0040521010075612145,
    "weighted_ngram_match_score": 0.08210967436686387,
    "syntax_match_score": 0.15428571428571428,
    "dataflow_match_score": 0.0
  },
  "codeBert_average_score": {
    "codeBert_average_precision": 0.6434360623359681,
    "codeBert_average_recall": 0.5209565991163254,
    "codeBert_average_f1": 0.5753152549266816
  }
}
{
  "model_name": "nisten/Biggie-SmoLlm-0.15B-Base",
  "total_evaluated": 50,
  "completion_evaluation": 0.0,
  "compilable_evaluation": 0.0,
  "code_bleu_average_score": {
    "bleu_score": 0.17950917251248646,
    "ngram_match_score": 0.0015557165295792947,
    "weighted_ngram_match_score": 0.003696996144319433,
    "syntax_match_score": 0.13661159547088128,
    "dataflow_match_score": 0.19617238190516595
  },
  "codeBert_average_score": {
    "codeBert_average_precision": 0.632478049993515,
    "codeBert_average_recall": 0.622841078042984,
    "codeBert_average_f1": 0.6270801889896392
  }
}
{
  "model_name": "gpt35",
  "total_evaluated": 50,
  "completion_evaluation": 0.62,
  "compilable_evaluation": 0.94,
  "code_bleu_average_score": {
    "bleu_score": 0.42385187239964894,
    "ngram_match_score": 0.2750511345641364,
    "weighted_ngram_match_score": 0.3299660563968335,
    "syntax_match_score": 0.6100411717948959,
    "dataflow_match_score": 0.4803491268427299
  },
  "codeBert_average_score": {
    "codeBert_average_precision": 0.8394056391716004,
    "codeBert_average_recall": 0.8221784663200379,
    "codeBert_average_f1": 0.8300538277626037
  }
}
{
  "model_name": "llama3-8b-8192",
  "total_evaluated": 50,
  "completion_evaluation": 0.0,
  "compilable_evaluation": 0.9,
  "code_bleu_average_score": {
    "bleu_score": 0.2690663154428746,
    "ngram_match_score": 0.02854552013295527,
    "weighted_ngram_match_score": 0.0495104010105122,
    "syntax_match_score": 0.08258223906830989,
    "dataflow_match_score": 0.07562710155972104
  },
  "codeBert_average_score": {
    "codeBert_average_precision": 0.15395407915115356,
    "codeBert_average_recall": 0.1400784170627594,
    "codeBert_average_f1": 0.14661067008972167
  }
}

{
  "model_name": "gemma-7b-it",
  "total_evaluated": 50,
  "completion_evaluation": 0.08,
  "compilable_evaluation": 0.5,
  "code_bleu_average_score": {
    "bleu_score": 0.3802555097846751,
    "ngram_match_score": 0.2049543597935755,
    "weighted_ngram_match_score": 0.3127010710006234,
    "syntax_match_score": 0.5030742760425299,
    "dataflow_match_score": 0.4202923323019715
  },
  "codeBert_average_score": {
    "codeBert_average_precision": 0.8241898334026336,
    "codeBert_average_recall": 0.7516354328393936,
    "codeBert_average_f1": 0.7845782321691513
  }
}
{
  "model_name": "gemma2-9b-it",
  "total_evaluated": 50,
  "completion_evaluation": 0.12,
  "compilable_evaluation": 0.54,
  "code_bleu_average_score": {
    "bleu_score": 0.43361317235957253,
    "ngram_match_score": 0.30303167786251584,
    "weighted_ngram_match_score": 0.3859229862410423,
    "syntax_match_score": 0.5676446640971137,
    "dataflow_match_score": 0.47785336123761807
  },
  "codeBert_average_score": {
    "codeBert_average_precision": 0.8667839765548706,
    "codeBert_average_recall": 0.8627540647983551,
    "codeBert_average_f1": 0.8639977657794953
  }
}
{
  "model_name": "llama3-groq-8b-8192-tool-use-preview",
  "total_evaluated": 50,
  "completion_evaluation": 0.08,
  "compilable_evaluation": 0.28,
  "code_bleu_average_score": {
    "bleu_score": 0.42449217169075043,
    "ngram_match_score": 0.2461529692907958,
    "weighted_ngram_match_score": 0.3608159498399295,
    "syntax_match_score": 0.5581916687177154,
    "dataflow_match_score": 0.512808098914561
  },
  "codeBert_average_score": {
    "codeBert_average_precision": 0.8366040289402008,
    "codeBert_average_recall": 0.7793141603469849,
    "codeBert_average_f1": 0.8062344622612
  }
}
{
  "model_name": "gemma-7b-it",
  "total_evaluated": 50,
  "completion_evaluation": 0.02,
  "compilable_evaluation": 0.3,
  "code_bleu_average_score": {
    "bleu_score": 0.3626252997302991,
    "ngram_match_score": 0.17803532997824537,
    "weighted_ngram_match_score": 0.29867993060972753,
    "syntax_match_score": 0.4945470705887908,
    "dataflow_match_score": 0.39923886774443273
  },
  "codeBert_average_score": {
    "codeBert_average_precision": 0.8092975306510926,
    "codeBert_average_recall": 0.7359192138910293,
    "codeBert_average_f1": 0.7691185611486435
  }
}
{
  "model_name": "llama3-groq-70b-8192-tool-use-preview",
  "total_evaluated": 50,
  "completion_evaluation": 0.06,
  "compilable_evaluation": 0.5,
  "code_bleu_average_score": {
    "bleu_score": 0.3518198134803873,
    "ngram_match_score": 0.1563619486839319,
    "weighted_ngram_match_score": 0.2128745882789884,
    "syntax_match_score": 0.39785560711613854,
    "dataflow_match_score": 0.3201871098424906
  },
  "codeBert_average_score": {
    "codeBert_average_precision": 0.7810989987850189,
    "codeBert_average_recall": 0.7321178805828095,
    "codeBert_average_f1": 0.7536820423603058
  }
}
{
  "model_name": "llama-3.1-70b-versatile",
  "total_evaluated": 50,
  "completion_evaluation": 0.06,
  "compilable_evaluation": 0.42,
  "code_bleu_average_score": {
    "bleu_score": 0.3813607369036434,
    "ngram_match_score": 0.20629237622231522,
    "weighted_ngram_match_score": 0.2945313081195772,
    "syntax_match_score": 0.4730340126137136,
    "dataflow_match_score": 0.43158525065896763
  },
  "codeBert_average_score": {
    "codeBert_average_precision": 0.7903066539764404,
    "codeBert_average_recall": 0.7392206037044525,
    "codeBert_average_f1": 0.7618382853269577
  }
}
{
  "model_name": "gemma2-9b-it",
  "total_evaluated": 50,
  "completion_evaluation": 0.02,
  "compilable_evaluation": 0.34,
  "code_bleu_average_score": {
    "bleu_score": 0.42211693306005676,
    "ngram_match_score": 0.2832977133745752,
    "weighted_ngram_match_score": 0.3547699563720069,
    "syntax_match_score": 0.5716486224599948,
    "dataflow_match_score": 0.4787514400336501
  },
  "codeBert_average_score": {
    "codeBert_average_precision": 0.8553845107555389,
    "codeBert_average_recall": 0.8600707733631134,
    "codeBert_average_f1": 0.856799931526184
  }
}
{
  "model_name": "llama3-70b-8192",
  "total_evaluated": 50,
  "completion_evaluation": 0.06,
  "compilable_evaluation": 0.44,
  "code_bleu_average_score": {
    "bleu_score": 0.3785911770277454,
    "ngram_match_score": 0.16267840781408827,
    "weighted_ngram_match_score": 0.27252025784800604,
    "syntax_match_score": 0.3979784279670535,
    "dataflow_match_score": 0.38118761448183386
  },
  "codeBert_average_score": {
    "codeBert_average_precision": 0.7730170464515687,
    "codeBert_average_recall": 0.6867396330833435,
    "codeBert_average_f1": 0.7251974833011627
  }
}
{
  "model_name": "llama3-8b-8192",
  "total_evaluated": 50,
  "completion_evaluation": 0.04,
  "compilable_evaluation": 0.44,
  "code_bleu_average_score": {
    "bleu_score": 0.34663959163320307,
    "ngram_match_score": 0.16097086872189703,
    "weighted_ngram_match_score": 0.24836445186316813,
    "syntax_match_score": 0.39882695684263647,
    "dataflow_match_score": 0.39839608910511076
  },
  "codeBert_average_score": {
    "codeBert_average_precision": 0.7843101060390473,
    "codeBert_average_recall": 0.7150187170505524,
    "codeBert_average_f1": 0.7472466695308685
  }
}
